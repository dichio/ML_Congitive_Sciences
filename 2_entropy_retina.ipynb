{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaea1bf29cb0d4ce",
   "metadata": {},
   "source": [
    "# Tutorial 2 ‚Äî Entropy and information in neural spike trains\n",
    "\n",
    "**Author:** [Vito Dichio](https://sites.google.com/view/vito-dichio/home),\n",
    "Fellow in AI, ENS‚ÄìPSL, Paris ‚Äî ‚úâÔ∏è vito.dichio@psl.eu\n",
    "\n",
    "**Course:** *Machine Learning: Theory and Applications*\n",
    "Master in Cognitive Sciences, ENS‚ÄìPSL ‚Äî Fall 2025/26 (Lecturer: Simona Cocco)\n",
    "\n",
    "**Format:** Practical Session (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f9e659db95364",
   "metadata": {},
   "source": [
    "#### Bibliography:\n",
    "[1] Cocco et al., *From Statistical Physics to Data-Driven Modelling: with Applications to Quantitative Biology*, Oxford University Press (2022)\n",
    "\n",
    "[2] de Ruyter van Steveninck et al., *Reproducibility and Variability in Neural Spike Trains*, Science 275, 1805‚Äì1808 (1997)\n",
    "\n",
    "[3] Koch et al., *Efficiency of Information Transmission by Retinal Ganglion Cells*, Current Biology 14, 1523‚Äì1530 (2004)\n",
    "\n",
    "[4] Schneidman et al., *Weak pairwise correlations imply strongly correlated network states in a neural population*, Nature 440, 7087 (2006)\n",
    "\n",
    "[5] Cover & Thomas, *Elements of information theory*. John Wiley & Sons (1999)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a9e82127e9899",
   "metadata": {},
   "source": [
    " #### üéØ Goals\n",
    "\n",
    "- Define and compute entropy for discrete distributions (in bits)\n",
    "- Bin spike trains and encode responses as binary words\n",
    "- Estimate *total entropy* and *noise entropy* and mutual information \n",
    "\n",
    "**Note** / The present notebook is based on the *Tutorial 2: entropy and information in neural spike trains* in [1], Section 2.4. An additional general reference for the mathematical background is also [5], Chapter 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2f9bd10b834fb",
   "metadata": {},
   "source": [
    "## Part A - Entropy of a Poisson spike train\n",
    "\n",
    "Consider a single neuron whose spikes follow a Poisson process with constant rate $\\theta$.  Let $N$ be the random variable denoting the number of spikes observed in a time window of length $T$.  Then $N$ is Poisson-distributed (see Tutorial 1):\n",
    "\n",
    "$$\n",
    "P_N(n \\mid \\lambda) = \\Pr[N = n \\mid \\lambda]\n",
    "= \\frac{\\lambda^n e^{-\\lambda}}{n!}, \\quad n=0,1,2,\\dots\n",
    "$$\n",
    "\n",
    "where $\\lambda = \\theta T$ is the **expected number of spikes** in the window.\n",
    "\n",
    "To analyze the spike train in a way that connects to information-theoretic quantities, we discretize the continuous time axis into non-overlapping bins of length $\\Delta t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b66d264e1aef3",
   "metadata": {},
   "source": [
    "#### üéØ Question A1.\n",
    "\n",
    "Let $\\sigma \\in \\{0,1\\}$ be the binary random variable defined for each bin of length $\\Delta t$ as follows: $\\sigma=0$ if there are no spikes in the bin, $\\sigma=1$ if there is **at least** one spike. This coarse-graining reduces the spike train to a binary sequence, which we can now analyze in terms of information content.\n",
    "\n",
    "Compute analytically:\n",
    "\n",
    "(i) the probability distribution $P_\\sigma$ as functions of $\\theta$ and $\\Delta t$. \n",
    "\n",
    "üí° **Hint:** Use the fact that $P_\\sigma(1) = 1 - P_N(0 \\mid \\theta, \\Delta t).$\n",
    "\n",
    "(ii) the entropy $S(P_{\\sigma})$ of the binary distribution $P_{\\sigma}$ and the entropy rate $S(P_{\\sigma})/\\Delta t$ (in bits per second)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e109cd4c2ce27fd",
   "metadata": {},
   "source": [
    "#### üéØ Question A2.\n",
    "Plot the entropy and entropy rate as functions of $\\Delta t$, for $\\theta = 2$ Hz, $4$ Hz, $8$ Hz.\n",
    "\n",
    "\n",
    "üí° **Hint:** Use of `scipy.special.xlogy(x, x)` to safely compute xlog(x) ensuring the correct handling of the case x=0, see [Scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.xlogy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d528d4bc699264",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B - Entropy and information in experimental spike trains\n",
    "\n",
    "We now move from the Poisson model to **real data**: the spiking activity of $L=40$ retinal ganglion cells, recorded during repeated presentations of a\n",
    "natural movie stimulus [4].  This is the same dataset used in *Tutorial 1, Part B*. You can load it directly from the file `data/dati2-berry.dat` using:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8ac3139577225b1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T10:40:54.914371Z",
     "start_time": "2025-10-04T10:40:54.838682Z"
    }
   },
   "source": [
    "# Number of neurons and stimulus repetitions\n",
    "import numpy as np\n",
    "\n",
    "N_rep  = 120\n",
    "T_stim  = 26.5   # duration (s) of one stimulus repetition\n",
    "\n",
    "def load_spike_data(path='data/dati2-berry.dat'):\n",
    "    \"\"\"\n",
    "    Load and parse spike-train data from Schneidman et al. (2006).\n",
    "    Each block in the file starts with a marker (4000), followed by\n",
    "    the neuron ID, then the spike times (s).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datas : list of np.ndarray\n",
    "        Spike times for each neuron (length L_neurons).\n",
    "    \"\"\"\n",
    "    raw = np.loadtxt(path)\n",
    "    chunks = np.split(raw, np.where(raw == 4000)[0][1:])\n",
    "    data = [chunk[2:] for chunk in chunks]  # drop marker + neuron ID\n",
    "    return data\n",
    "\n",
    "# Load spike trains\n",
    "data = load_spike_data()\n",
    "\n",
    "N_neurons = len(data)\n",
    "T_total = T_stim*N_rep # total recording time\n",
    "\n",
    "print(f\"Loaded {N_neurons} neurons, total duration {T_total:.1f}s\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 neurons, total duration 3180.0s\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "77d143ce10e98d2d",
   "metadata": {},
   "source": [
    "#### üéØ Question B1. \n",
    "Discretize the spike train of each neuron into bins of length $\\Delta t = 0.01\\ \\mathrm{s}$. Define a binary variable $\\sigma \\in \\{0,1\\}$ for each time bin, with $\\sigma=1$ if at least one spike occurs, and $\\sigma=0$ otherwise. The result should be a binary array `data_bin` of shape\n",
    "$(N_{\\text{neurons}},\\, N_{\\text{bins}})$, where $N_{\\text{bins}} = N_{\\text{rep}} T_{\\text{stim}} / \\Delta t$.\n",
    "\n",
    "üí° **Hint:** \n",
    "Given a spike time `t`, the spike time-bin index can be found by `(t/dt).astype(int)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78b029-1e4f-4847-9280-bfdd651bdd4b",
   "metadata": {},
   "source": [
    "### Word-based encoding\n",
    "\n",
    "To quantify **reproducibility** and **information content** of neural responses, we need to move beyond single spikes or bins.   Instead, we represent the spike train as a sequence of **binary words**, which capture temporal patterns of activity over short time windows.  This approach allows us to compare not just firing rates, but the *structure* and *variability* of neural responses across repeated stimulus presentations.\n",
    "\n",
    "Concretely, we split the spike train into non-overlapping binary words, whose associated random variable is $W$, each spanning a time window of length $L_W$ seconds and therefore extending over $l_W = L_W/\\Delta t$ consecutive time bins. Let us call $\\mathcal{W}$ the space of all possible binary words of length $l_W$. The number of possible words is: $N_{\\mathrm{W}} = 2^{l_{W}}$.\n",
    "\n",
    "\n",
    "**Example**. consider the following spike train, where each bin corresponds to $\\Delta t = 0.01\\ \\mathrm{s}$:\n",
    "\n",
    "[0,1,1,1,0,0,1,0,1,0,0,0,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,(...)]\n",
    "\n",
    "This can be split into words of length $L_W = 0.1\\ \\mathrm{s}$ (i.e. $l_W=10$ bins):\n",
    "\n",
    "  - word 1: [0,1,1,1,0,0,1,0,1,0]\n",
    "  - word 2: [0,0,0,1,0,1,0,1,0,1]\n",
    "  - word 3: [0,1,0,1,0,1,0,1,0,1]\n",
    "  - (...)\n",
    "\n",
    "There are $N_W= 2^{l_W} = 2^{10} = 1024$ possible binary words of length $l_W$.\n",
    "\n",
    "Given a spike train, we can estimate:\n",
    "- The **total distribution** $P(W)$, i.e., relative frequency of each word across the full recording.\n",
    "- The corresponding **total entropy**:\n",
    "  $$\n",
    "  S_{\\mathrm{total}} \\equiv S(W) = -\\sum_{w\\in\\mathcal{W}} P(w) \\log_2 P(w) \\quad \\text{bits}.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e7bd46-ded6-4320-8639-4ed9a441d1a8",
   "metadata": {},
   "source": [
    "#### üéØ Question B2. \n",
    "For the first neuron, compute and print the total entropy $S_{\\mathrm{total}}$ for $L_W = 0.1\\ \\mathrm{s}$ and $\\Delta t = 0.01\\ \\mathrm{s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4307d90c9cbdc",
   "metadata": {},
   "source": [
    "##### üí° Hint for Question B2\n",
    "\n",
    "Code a function `compute_total_entropy` which computes the total entropy of **a single spike train** (one neuron) by applying the following algorithm:\n",
    "\n",
    "1. **Split into words**.\n",
    "   Cut the (binned) spike train into consecutive, **non-overlapping** blocks of length `l_W` (bins).\n",
    "   Each block is one binary word.\n",
    "   *(Use the provided `split_nonoverlapping_words`.)*\n",
    "2. **Map words to integers**.\n",
    "   Convert each binary word to an integer in `[0, 2**l_W - 1]`, this will be the word ID.\n",
    "   *(Use the provided `bits_to_int`)*\n",
    "3. **Build empirical distribution**.\n",
    "   Count occurrences of each word ID and normalize to obtain the probability vector\n",
    "   $P(W)$ of length `2**l_W`.\n",
    "   One possible choice is to use `np.bincount` with `minlength=2**l_W`, then divide by the total number of occurrences, see [Numpy documentation](https://numpy.org/doc/2.1/reference/generated/numpy.bincount.html))\n",
    "4. **Compute entropy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f190335aa917e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T12:12:36.630189Z",
     "start_time": "2025-09-22T12:12:36.627307Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def split_nonoverlapping_words(spike_train, l_W):\n",
    "    \"\"\"\n",
    "    Split a 1D binary sequence (spike train for a single neuron) into consecutive **non-overlapping** words of length l_W.\n",
    "    Truncates any trailing incomplete block.\n",
    "    Returns: (n_words x l_W) array of 0/1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the length to truncate so only full words are kept\n",
    "    n_words = (len(spike_train) // l_W) * l_W\n",
    "    # Truncate the spike train to a multiple of l_W\n",
    "    row = spike_train[:n_words]\n",
    "\n",
    "    # Reshape into (n_words, l_W)\n",
    "    return row.reshape(-1, l_W)\n",
    "\n",
    "\n",
    "def bits_to_int(words, l_W):\n",
    "    \"\"\"\n",
    "    Convert an array of binary words (shape: n_words x l_W) into integers in [0, 2^l_W) - l_W is the legth (in bins) of the words.\n",
    "    Vectorized: uses bit weights instead of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an array of bit weights: [2^(word_bins-1), ..., 2^0]\n",
    "    weights = np.power(2, np.arange(l_W-1, -1, -1))\n",
    "\n",
    "    # Compute the integer value for each word by dot product with weights.\n",
    "    # For each row: sum(word[i] * weights[i]) over all bits.\n",
    "    return (words @ weights).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120de808-77e2-4548-8251-e62cb8f4c7b8",
   "metadata": {},
   "source": [
    "### Noise entropy and information\n",
    "\n",
    "To assess how reproducible the neural responses are across repeated presentations of the same stimulus, we align trials and examine the distribution of words at each **word bin** - that is, each segment of the stimulus cycle of duration $L_W$ seconds, corresponding to a word of length $l_W$ time bins.\n",
    "\n",
    "- If the neuron responds deterministically, the same word appears at each word bin on every trial.\n",
    "- If responses vary across trials, the distribution of words at a given word bin broadens, reflecting noise in the response.\n",
    "\n",
    "This motivates the definition of the **noise entropy**: the average uncertainty (entropy) of the word distribution when the word bin is fixed. Informally, it is the variability that does not come from that the variation of the stimulus.\n",
    "\n",
    "\n",
    "Formally, let $B$ be the random variable denoting the **word bin** within a stimulus cycle. Here, each word bin corresponds to a specific stimulus, that is a specific moment of the replayed movie. $B$ takes values $b\\in\\mathcal{B}=\\{0, 1, \\ldots, N_B-1\\}$, where $N_B = T_{\\text{stim}}/L_W$ is the number of words that fit in one stimulus repetition. Since all word bins are equally likely, $P(B=b) = 1/N_B$. The conditional entropy of words given the word bin is defined as follows\n",
    "\n",
    "  $$\n",
    "    S(W\\mid B) \\equiv \\sum_{b\\in\\mathcal{B}} p(b) S(W\\mid B=b) = -\\frac{1}{N_B} \\sum_{b\\in\\mathcal{B}} \\sum_{w\\in\\mathcal{W}} P(w \\mid b)\\,\\log_2 P(w \\mid b).\n",
    "  $$\n",
    "\n",
    "Here, $P(w\\mid b)$ is the empirical distribution of words at word bin $b$ across stimulus repeats. This latter entropy is also called **noise entropy** $S_{\\text{noise}}$ since it reflects the non-deterministic response to the stimulus.\n",
    "\n",
    "- If responses are perfectly reproducible, each $P(w \\mid b)$ is a delta distribution, so $S_{\\text{noise}}=0$.\n",
    "- Larger $S_{\\text{noise}}$ means more variability across trials, i.e. less reproducibility.\n",
    "\n",
    "The **mutual information** between the stimulus (represented by the word bin $B$) and the response (the word $W$) is defined as:\n",
    "\n",
    "$$\n",
    "I(W; B) \\equiv S(W) - S(W \\mid B)\n",
    "$$\n",
    "\n",
    "Thus, the **information** carried by the spike train about the stimulus is estimated as:\n",
    "\n",
    "$$\n",
    "I = S_{\\text{total}} - S_{\\text{noise}} \\quad \\text{(bits)}.\n",
    "$$\n",
    "\n",
    "- If \\(I = 0\\), the neural response carries no information about the stimulus (responses are independent of stimulus timing).\n",
    "- Larger \\(I\\) indicates that spike patterns are more reproducible and stimulus-locked, hence more informative about the stimulus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3be327-58d8-4eb3-8de7-4f14d14e33ee",
   "metadata": {},
   "source": [
    "#### üéØ Question B3. \n",
    "Compute the noise entropy $S_{\\text{noise}}$ and information $I$ for the first neuron, using $L_W = 0.1\\ \\mathrm{s}$ and $\\Delta t = 0.01\\ \\mathrm{s}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcbe407-34dc-4960-bb7f-f64f9f1252c8",
   "metadata": {},
   "source": [
    "##### üí° Hint for Question B3.\n",
    "Implement `compute_noise_entropy` for a single spike train:\n",
    "\n",
    "1. **Reshape trials** to shape $(N_{\\text{rep}},\\, N_{\\text{bins per stim}})$, one row per repetition.\n",
    "2. **Split each repetition into words** (length $l_W$) and map to integer IDs as done before (use the helpers).\n",
    "3. **Group by word bin** $b$: at each position in the stimulus cycle, collect words across repetitions.\n",
    "4. **Compute entropy** at each $b$: estimate $P(W\\mid B=b)$ and its entropy (bits).\n",
    "5. **Average** across bins to obtain $S_{\\text{noise}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f84f2-39ab-410d-8f0b-dfb77ac223c4",
   "metadata": {},
   "source": [
    "#### üéØ Question B4. \n",
    "Generalise the previous computations to all neurons and print the results in a table. Plot these three quantities ($S_{\\text{total}}$, $S_{\\text{noise}}$, $I$) as functions of the spiking frequency of each neuron. Compare to the total entropy rate of this process with the one of the binary process from Part A, briefly discuss why the entropy rate is the relevant quantity for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7ae9b-59e7-408a-add7-3d91a83e813d",
   "metadata": {},
   "source": [
    "##### üí° Hint for Question B4.\n",
    "Use the functions `compute_total_entropy(spike_train, ...)` (from B.2) and `compute_noise_entropy(spike_train, ...)` (from B.3) and apply them to **all neurons at once** by using `np.apply_along_axis`. See the [NumPy documentation](https://numpy.org/doc/stable/reference/generated/numpy.apply_along_axis.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f9667-25a0-4410-a57a-8813e0e71bb8",
   "metadata": {},
   "source": [
    "#### üéØ Question B5. \n",
    "For the first neuron, compute the **noise entropy** $S_{\\mathrm{noise}}(b)$ **for each word bin** $b$ within the stimulus cycle (without averaging across bins).  \n",
    "\n",
    "Plot these entropy values as a function of the **average firing rate** in each bin. On the same figure, overlay the theoretical prediction for a Poisson process with the same (word) bin rates, computed over $l_W$ time bins, namely $l_W \\, S(P_{\\sigma})$, where $S(P_{\\sigma})$ is the binary entropy introduced in Part A1.\n",
    "\n",
    "**Discussion:** Comment on how the measured noise entropy compares to the Poisson prediction: what does this tell us about the structure of the neural responses relative to a homogeneous Poisson model? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ae0b4-543a-4e48-b59d-cb02952a7d81",
   "metadata": {},
   "source": [
    "##### üí° Hint for Question B5.\n",
    "In order to compute the average firing rate in each bin, start from the raw spike times, then apply `np.unique` to `(spike_times % T_stim) // L_W`, see [Numpy documentation](https://numpy.org/devdocs/reference/generated/numpy.unique.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e93ef668bc098",
   "metadata": {},
   "source": [
    "#### üéØ Question B6 (Optional). \n",
    "Investigate how the estimated information $I$ depends on two key parameters:\n",
    "\n",
    "1. The number of stimulus repetitions $N_{\\text{rep}}$.\n",
    "2. The word duration $L_W$ (in seconds).\n",
    "\n",
    "For the first neuron:\n",
    "- Compute the information $I$ as a function of $N_{\\text{rep}}$ and $L_W$.\n",
    "- Express the result both as **bits per word** and as **bits per second**.\n",
    "- Examine whether the information per second converges as $N_{\\text{rep}}$ and $L_W$ increase.\n",
    "\n",
    "**Discussion:** Are the available data $(N_{\\text{rep}}=120$ and word size $L_W = 0.1\\,\\mathrm{s}$) sufficient to obtain a reliable estimate of the information rate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
