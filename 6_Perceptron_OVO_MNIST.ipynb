{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27e74ea99fe5ee3",
   "metadata": {},
   "source": [
    "# Tutorial 6 ‚Äî Multi-Class Perceptron\n",
    "\n",
    "**Author:** [Vito Dichio](https://sites.google.com/view/vito-dichio/home),\n",
    "Fellow in AI, ENS‚ÄìPSL, Paris ‚Äî ‚úâÔ∏è vito.dichio@psl.eu\n",
    "\n",
    "**Course:** *Machine Learning: Theory and Applications*\n",
    "Master in Cognitive Sciences, ENS‚ÄìPSL ‚Äî Fall 2025/26 (Lecturer: Simona Cocco)\n",
    "\n",
    "**Format:** Practical Session (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18c7e623cee5636",
   "metadata": {},
   "source": [
    "#### Bibliography:\n",
    "\n",
    "[1] Cocco et al., *From Statistical Physics to Data-Driven Modelling: with Applications to Quantitative Biology*, Oxford University Press (2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb523991f84080",
   "metadata": {},
   "source": [
    "# Part A: Critical Capacity of the Perceptron\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "In Tutorial 5, you applied the perceptron to MNIST digits (0 vs 1), which have structure and patterns. But what happens when we try to memorize **completely random associations** between inputs and outputs?\n",
    "\n",
    "We consider $M$ **binary random patterns:** both inputs $x^{(m)} \\in \\{0, 1\\}^L$ and labels $y^{(m)} \\in \\{0, 1\\}$ are drawn uniformly at random, with no correlation. Here, $L$ is the dimension of the input space (number of features) and $M$ is the number of patterns to classify.\n",
    "\n",
    "Consider a perceptron learning problem with **loading parameter** $\\alpha = M/L$. For random binary patterns in the double infinite size limit $M, L \\to \\infty$ with fixed ratio $\\alpha = M/L$, the **critical capacity** is $\\alpha_c \\approx 2$. This means:\n",
    "- **$\\alpha < \\alpha_c$**: Random patterns are typically linearly separable $\\Rightarrow$ perceptron converges\n",
    "- **$\\alpha > \\alpha_c$**: Patterns typically NOT linearly separable $\\Rightarrow$ perceptron fails to converge\n",
    "\n",
    "For finite $L$ the transition is not infinitely sharp but occurs over a range around $\\alpha_c \\approx 2$. The transition becomes sharper as $L$ increases (see [1] Sec. 7.2.2, Figure 7.5 for illustration of finite-size effects).\n",
    "\n",
    "**Your task:** Empirically verify this theorem by training perceptrons on random patterns for various $\\alpha$ values and observing the convergence rate transition at $\\alpha_c \\approx 2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641fa2c9863f51e",
   "metadata": {},
   "source": [
    "### üéØ Question A.1: Generate Random Patterns\n",
    "\n",
    "Implement a function called `generate_random_patterns` to generate random binary patterns and labels. The function should take as input the number of patterns `M`, the dimension `L`, and a random seed for reproducibility.\n",
    "\n",
    "**Requirements:**\n",
    "- Use `np.random.default_rng(seed)` for the random number generator\n",
    "- Both patterns and labels should be uniformly random in $\\{0, 1\\}$\n",
    "- Ensure not all labels are identical (redraw if needed, max 10 attempts)\n",
    "- Each call with the same seed should produce identical results\n",
    "\n",
    "Test your function for reproducibility by calling it twice with the same seed and checking that the outputs match. Use as `M`, `L` and `seed` the day, month and year of your **oral exam date** (e.g., for December 18, 2025 use `M=18`, `L=12`, `seed=2025`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e2f649975df55",
   "metadata": {},
   "source": [
    "### üéØ Question A.2: Test Capacity for Single Configuration\n",
    "\n",
    "Implement a function `test_capacity` that tests perceptron capacity for a range of $\\alpha$ values with fixed $L$. The function should take as input:\n",
    "- `alpha_values`: array of $\\alpha$ values to test\n",
    "- `L`: dimension of input patterns\n",
    "- `n_trials`: number of random trials per $\\alpha$\n",
    "- `max_updates`: maximum perceptron updates per trial\n",
    "\n",
    "**Algorithm for each $\\alpha$:**\n",
    "1. Compute the number of patterns given `alpha` and `L`\n",
    "2. For each trial:\n",
    "   - Generate random patterns **using as seed the trial index** for reproducibility\n",
    "   - Train perceptron using the perceptron learning algorithm with a maximum of `max_updates` updates\n",
    "   - Predict and compute **training error**\n",
    "   - Count as \"converged\" if training error is $\\approx 0$  up to a difference of `1e-6` (use `np.isclose`)\n",
    "3. Compute convergence rate: `converged_count / n_trials`\n",
    "4. Compute mean training error across all trials\n",
    "\n",
    "The function should finally return a DataFrame with columns: `alpha`, `convergence-rate`, `training error` as output.\n",
    "\n",
    "**Test your function:**\n",
    "```python\n",
    "# Quick test with small parameters\n",
    "alpha_test = np.linspace(0.25, 4.0, 16)\n",
    "df_test = test_capacity(alpha_test, L=10, n_trials=10, max_updates=1000)\n",
    "\n",
    "print(df_test)\n",
    "# Should show high convergence rate for Œ± < 2, lower for Œ± > 2\n",
    "```\n",
    "\n",
    "üí° **Hint:** You may want to reuse the functions  `perceptron_signed`, `linear_classifier_predict` and `compute_error_rate` for an efficient implementation of the perceptron training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221f80fbdab70b5",
   "metadata": {},
   "source": [
    "### üéØ Question A.3: Run Capacity Experiments\n",
    "\n",
    "Run systematic experiments to verify the critical capacity $\\alpha_c \\approx 2$ and investigate how dimension $L$ and training time affect the transition.\n",
    "\n",
    "#### (i) Run experiments\n",
    "\n",
    "**Experimental parameters:**\n",
    "- `alpha_values = np.linspace(0.5, 3.0, 11)` - Range of loading parameters\n",
    "- `n_trials = 100` - Number of independent trials per configuration\n",
    "- Test all 4 combinations of:\n",
    "  - `L = 10, 50` - Pattern dimensions\n",
    "  - `max_updates = 1000, 5000` - Training budget\n",
    "\n",
    "**Task:** For each of the 4 configurations, run `test_capacity` and store results. You should obtain 4 DataFrames, one per combination of (`L`, `max_updates`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d6f9ba12cc98d",
   "metadata": {},
   "source": [
    "#### (ii) Plot the results\n",
    "\n",
    "Create **two plots** showing all 4 experimental configurations: in the first plot, show convergence rate vs $\\alpha$; in the second plot, show mean training error vs $\\alpha$.\n",
    "\n",
    "Mark the theoretical critical capacity $\\alpha_c \\approx 2$ with a vertical dashed black line in both plots. Use distinct colors or markers for each configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a9a0770bb75",
   "metadata": {},
   "source": [
    "#### (iii) Interpret the results\n",
    "\n",
    "Comment on your findings from the experiments and plots. Address the following points:\n",
    "\n",
    "1. **Critical capacity identification:** At what $\\alpha$ value does the convergence rate drop most sharply for each configuration? How close are these values to the theoretical prediction $\\alpha_c = 2$?\n",
    "\n",
    "2. **Finite-size effects:** Compare the transition width for $L=10$ vs $L=50$. Does the transition become sharper for larger $L$, as predicted by theory? Cf Figure 7.5 in [1].\n",
    "\n",
    "3. **Role of training budget:** Does increasing `max_updates` from 1000 to 5000 change the results? Specifically, does it affect the convergence rate near the critical region? If so, in which direction and why?\n",
    "\n",
    "4. **Error behavior:** Examine the training error plots:\n",
    "   - How does mean training error evolve as $\\alpha$ increases?\n",
    "   - For $\\alpha > 2$, why is the error non-zero even though the perceptron runs for many updates?\n",
    "   - Compare error curves for different `max_updates`: does training longer always help?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5aab6daadc787",
   "metadata": {},
   "source": [
    "# Part B: Multi-Class Classification with One-vs-One Perceptrons\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Tutorial 5, you trained a perceptron to distinguish between two MNIST digits (0 vs 1). But what if we want to classify all 10 digits (0-9)? This requires extending binary classification to **multi-class classification**.\n",
    "\n",
    "The strategy we will use is **One-vs-One (OVO)**, where we train a separate binary classifier for each pair of classes. During prediction, each classifier votes for one of its two classes, and the class with the most votes wins. In this part, you will implement the **One-vs-One (OVO)** strategy and analyze its performance on the full MNIST dataset. The entire dataset contains 60,000 training samples and 10,000 test samples of handwritten digits (0-9) and can be loaded using the`load_mnist_data` function from Tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9213326c3b5c2",
   "metadata": {},
   "source": [
    "### üéØ Question B.1: Preliminaries\n",
    "\n",
    "Before implementing the OVO system, we need to prepare our data and ensure the perceptron can handle arbitrary class labels.\n",
    "\n",
    "#### (i) Load MNIST Data\n",
    "\n",
    "Load the full MNIST dataset (all 10 digits) and flatten the images. You should obtain:\n",
    "- `x`: shape `(70000, 784)` - flattened training images\n",
    "- `y`: shape `(70000,)` - training labels (0-9)\n",
    "\n",
    "üí° **Hint:** You may use the `load_mnist_data` and `flatten_images` functions from Tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41965980b3e0adef",
   "metadata": {},
   "source": [
    "#### (ii) Split MNIST into training and test sets\n",
    "\n",
    "After implementing, apply it to the MNIST images and then split it into training and test sets (60,000 train, 10,000 test).\n",
    "\n",
    "üí° **Hint:** You can use the `split_train_test` function from Tutorial 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1b826a46f9a1f",
   "metadata": {},
   "source": [
    "#### (iii) Make perceptron label-agnostic\n",
    "\n",
    "The `perceptron_signed` and `linear_classifier_predict` functions from Tutorial 4 assume labels are exactly 0 and 1 (or -1 and +1). For OVO, we need to classify any pair of digits (e.g., 1 vs 7).\n",
    "\n",
    "**Task:** Modify the perceptron function and prediction function to handle arbitrary binary labels by adding `pos_label` and `neg_label` parameters.\n",
    "\n",
    "**For `perceptron_signed`:**\n",
    "- Add a `pos_label` parameter (default: 1)\n",
    "- Before creating signed patterns, map the input labels: `pos_label ‚Üí +1`, other label ‚Üí `-1`. Use `np.where` for this mapping.\n",
    "\n",
    "**For `linear_classifier_predict`:**\n",
    "- Add `pos_label` (default: 1) and `neg_label` (default: 0) parameters\n",
    "- If score > theta: predict `pos_label`, otherwise predict `neg_label`\n",
    "\n",
    "With these modifications, you can train on any digit pair by specifying which digit is the \"positive\" class. For example, `perceptron_signed(X, y, pos_label=1)` will treat 1 as +1 and the other digit (e.g., 7) as -1 internally, while still returning predictions in the original labels (1 and 7).\n",
    "\n",
    "**Verify your implementation:**\n",
    "\n",
    "```python\n",
    "pos_label = 1\n",
    "neg_label = 7\n",
    "\n",
    "mask = (y_train == pos_label) | (y_train == neg_label)\n",
    "x_pair = x_train[mask]\n",
    "y_pair = y_train[mask]\n",
    "\n",
    "# Train perceptron treating 8 as positive class\n",
    "result = perceptron_signed(x_pair, y_pair, pos_label=pos_label, max_updates=2000)\n",
    "\n",
    "# Predict using consistent label mapping\n",
    "y_pred = linear_classifier_predict(x_pair, result['J'], theta=0,  pos_label=pos_label, neg_label=neg_label)\n",
    "\n",
    "# Compute error\n",
    "error = compute_error_rate(y_pair, y_pred, verbose=True)\n",
    "print(f\"Training error for {pos_label} vs {neg_label}: {error*100:.2f}%\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9c2f66c9d3b43",
   "metadata": {},
   "source": [
    "### üéØ Question B.2: Train One-vs-One Perceptrons\n",
    "\n",
    "The **One-vs-One (OVO)** strategy decomposes multi-class classification into multiple binary problems. For K classes, we train all possible pairwise classifiers:\n",
    "\n",
    "$$\\text{Number of classifiers} = \\binom{K}{2} = \\frac{K(K-1)}{2}$$\n",
    "\n",
    "For MNIST with K=10 digits, this means training **45 binary perceptrons**:\n",
    "- (0,1), (0,2), ..., (0,9): 9 classifiers involving digit 0\n",
    "- (1,2), (1,3), ..., (1,9): 8 classifiers involving digit 1\n",
    "- ...\n",
    "- (8,9): 1 classifier for the last pair\n",
    "\n",
    "Each binary classifier learns to distinguish between two specific digits using only the training samples from those two classes. For example, the (1,7) classifier is trained only on images of 1s and 7s, ignoring all other digits.\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Implement a function `train_ovo_perceptron` that takes as input the training data `X_train`, `y_train`, and trains an OVO perceptron system (45 binary classifiers) by calling your modified `perceptron_signed` function for each pair of classes. Use `max_updates=2000` for each binary classifier.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Extract all unique class labels (digits) from `y_train` (use `np.unique`)\n",
    "2. Initialize a data structure to store all trained classifiers\n",
    "3. For each pair of classes (i, j) where i < j:\n",
    "   - Extract training samples belonging to class i or class j (see example in B.1.iii)\n",
    "   - Train a binary perceptron on this subset using `perceptron_signed`\n",
    "   - Store the resulting weights and label information\n",
    "4. Return the collection of all trained classifiers\n",
    "\n",
    "\n",
    "üí°**Storage strategy:**\n",
    "\n",
    "You need to store 45 trained classifiers in a way to save for each of them:\n",
    "- The pair of classes it distinguishes and which of them is positive/negative\n",
    "- The trained weight vector\n",
    "\n",
    "Possible approaches include using dictionaries with tuple keys, lists of tuples, or any other structure you find appropriate. Think about how you will later retrieve and use these classifiers for prediction.\n",
    "\n",
    "‚ö†Ô∏è **Computational note:** Training on the full dataset with `max_updates=2000` may take **3-5 minutes** on a standard CPU. Start with smaller values to develop your code and debug, then run the full training once you're confident your implementation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d0e0439b0c714",
   "metadata": {},
   "source": [
    "### üéØ Question B.3: Predict with Majority Voting\n",
    "\n",
    "#### Voting Mechanism\n",
    "\n",
    "Once all binary classifiers are trained, we need a strategy to combine their predictions for multi-class classification. The **One-vs-One voting** works as follows:\n",
    "\n",
    "**For each test sample:**\n",
    "1. Each of the K(K-1)/2 binary classifiers predicts one of its two classes\n",
    "2. Count votes: each class accumulates votes from all classifiers that involve it\n",
    "3. Predict the class with the most votes (winner-takes-all)\n",
    "\n",
    "**Example for a sample that might be \"7\":**\n",
    "- Classifier (0,7) ‚Üí votes for 7\n",
    "- Classifier (1,7) ‚Üí votes for 7\n",
    "- Classifier (2,7) ‚Üí votes for 7\n",
    "- ...\n",
    "- Classifier (7,9) ‚Üí might vote for 9 (misclassification)\n",
    "- **Result:** Class 7 receives 8 votes, class 9 receives 1 vote ‚Üí predict 7\n",
    "\n",
    "Note that for K=10 classes, each sample receives exactly 45 votes (one from each binary classifier). For a given sample, each class can receive at most K-1=9 votes (from the 9 classifiers that involve that class).\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Implement `predict_ovo(X, classifiers)` that returns predicted class labels using majority voting.\n",
    "\n",
    "üí° **Pseudocode:**\n",
    "```\n",
    "N ‚Üê number of samples in X\n",
    "\n",
    "Extract all unique digit labels from your stored classifiers ‚Üí digits (sorted list)\n",
    "\n",
    "K ‚Üê number of unique digits\n",
    "\n",
    "Initialize vote matrix: votes (dimensions N x K) with zeros\n",
    "\n",
    "FOR each stored binary classifier:\n",
    "    Retrieve: weights, positive digit label, negative digit label\n",
    "\n",
    "    y_pred ‚Üê linear_classifier_predict(X, weights, pos_label, neg_label)\n",
    "\n",
    "    idx_pos ‚Üê index of positive digit in digits list\n",
    "    idx_neg ‚Üê index of negative digit in digits list\n",
    "\n",
    "    # Add votes (vectorized or loop over samples)\n",
    "    FOR those sample indices n where y_pred[n] == positive digit:\n",
    "        votes[n, idx_pos] += 1\n",
    "    FOR those sample indices n where y_pred[n] == negative digit:\n",
    "        votes[n, idx_neg] += 1\n",
    "\n",
    "# Winner-takes-all: find digit with most votes for each sample\n",
    "FOR each sample n:\n",
    "    winning_index ‚Üê argmax of the row votes[n, :]\n",
    "    predictions[n] ‚Üê digits[winning_index]\n",
    "\n",
    "RETURN predictions\n",
    "```\n",
    "\n",
    "Run the `predict_ovo` function on both the **train** and the **test** set and compute the overall accuracy by comparing with true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798ef9365f5e02a",
   "metadata": {},
   "source": [
    "### üéØ Question B.4: Confusion Matrix Analysis\n",
    "\n",
    "#### (i) Visualize Results\n",
    "\n",
    "A **confusion matrix** shows how often each true digit is predicted as each possible digit. The entry in row $i$, column $j$ shows how many times digit $i$ was predicted as digit $j$.\n",
    "\n",
    "- **Diagonal entries**: correct predictions (true label = predicted label)\n",
    "- **Off-diagonal entries**: confusions (true label ‚â† predicted label)\n",
    "\n",
    "**Task:** Implement `plot_confusion_matrix(y_true, y_pred, ...)` that:\n",
    "- Computes the confusion matrix using `sklearn.metrics.confusion_matrix` (See [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html))\n",
    "- Creates a heatmap visualization with:\n",
    "  - Color intensity showing number of samples\n",
    "  - Axis labels indicating true (rows) vs predicted (columns) digits\n",
    "\n",
    "**Generate confusion matrices for both train and test sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5743a24b6205f",
   "metadata": {},
   "source": [
    "### (ii) Interpret the Results\n",
    "\n",
    "Analyze the confusion matrices and address the following questions in your report:\n",
    "\n",
    "**1. Overall Performance**\n",
    "\n",
    "- **Best performers:** Which digits have the highest diagonal values (most correct predictions)? Is this consistent with Tutorial 4 results for digits 0 and 1?\n",
    "\n",
    "- **Per-digit accuracy:** How would you compute accuracy for each individual digit from the confusion matrix? Which digits have the lowest accuracy and why might this be? What would you suggest to improve performance on these digits?\n",
    "\n",
    "- **Generalization:** Compare train vs test confusion matrices. Are the error patterns similar? Does the model generalize well, or do you observe overfitting for certain digits?\n",
    "\n",
    "**2. Confusion Patterns and Symmetry**\n",
    "\n",
    "- **Major confusions:** Identify the largest off-diagonal entries (most common mistakes). Speculate on why this confusion occurs based on visual similarity\n",
    "\n",
    "- **Asymmetric confusions:** Examine whether the confusion matrix is generally not symmetric.  Identify at least two clear asymmetric pairs where confusion[i,j] >> confusion[j,i] and explain why these asymmetries might occur.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b35bf68981e19",
   "metadata": {},
   "source": [
    "## ‚ûï Optional (Bonus) Questions\n",
    "\n",
    "### üéØ B.5 The Role of the Bias Term\n",
    "\n",
    "In Tutorial 5, the perceptron learned decision boundaries of the form $J \\cdot x = 0$, which must pass through the origin. For many digit pairs, the optimal separating hyperplane needs to be shifted away from the origin. Adding a bias feature $x \\rightarrow [x; 1]$ allows the perceptron to learn boundaries of the form $J \\cdot x + b = 0$, where the weight corresponding to the bias feature acts as $b$. See the discussion in [1]. Sec. 7.1.1, p. 138.\n",
    "\n",
    "**Task:**  Implement a function `add_bias_feature(x)` that appends a constant feature (value = 1) to each sample. The function should take an array of shape `(N, features)` and return an array of shape `(N, features+1)` where the last column is all ones.\n",
    "\n",
    "Add the bonus bias to the MNIST data before splitting into train/test sets. Then, retrain the OVO perceptron system and evaluate performance again. What changes do you observe in accuracy and confusion matrices compared to the no-bias case? Are the (mis-)classification patterns similar or different?\n",
    "\n",
    "### üéØ B.6 The Role of Training Budget\n",
    "\n",
    "Investigate how increasing the `max_updates` parameter during training affects the OVO perceptron performance. Retrain the OVO system with a higher `max_updates=20000` and evaluate accuracy and confusion matrices again. Does training longer lead to significant improvements? Are certain digits more affected by the increased training budget than others? Warining: This may take **10-20** minutes to run!\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
