{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cd6a1aed07b15f",
   "metadata": {},
   "source": [
    "# Tutorial 5 ‚Äî Perceptron\n",
    "\n",
    "\n",
    "**Author:** [Vito Dichio](https://sites.google.com/view/vito-dichio/home),\n",
    "Fellow in AI, ENS‚ÄìPSL, Paris ‚Äî ‚úâÔ∏è vito.dichio@psl.eu\n",
    "\n",
    "**Course:** *Machine Learning: Theory and Applications*\n",
    "Master in Cognitive Sciences, ENS‚ÄìPSL ‚Äî Fall 2025/26 (Lecturer: Simona Cocco)\n",
    "\n",
    "**Format:** Practical Session (TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfcaa201d73258",
   "metadata": {},
   "source": [
    "#### Bibliography:\n",
    "[1] Rosenblatt, F. (1958). *The Perceptron: A probabilistic model for information storage and organization in the brain*. *Psychological Review*, 65(6), 386‚Äì408.\n",
    "\n",
    "[2] LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). *Gradient-based learning applied to document recognition*. *Proceedings of the IEEE*, 86(11), 2278‚Äì2324.\n",
    "\n",
    "[3] Bishop, C. M., & Bishop, H. (2023). Deep learning: Foundations and concepts. Springer Nature.\n",
    "\n",
    "[4] Cocco et al., *From Statistical Physics to Data-Driven Modelling: with Applications to Quantitative Biology*, Oxford University Press (2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c2b17018cbfba",
   "metadata": {},
   "source": [
    "#### üéØ **Goals**\n",
    "\n",
    "- Understand the principles of **linear classification** for supervised learning.\n",
    "- Implement the **Perceptron algorithm**, one of the earliest and simplest neural network models.\n",
    "- Apply the Perceptron to the **MNIST handwritten digits** dataset and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64092ce4b856e2c0",
   "metadata": {},
   "source": [
    "###  **Context and data**\n",
    "\n",
    "\n",
    "This tutorial introduces the **supervised learning** framework ‚Äî a central paradigm in machine learning where a model learns to map inputs to known outputs from a set of labeled examples.\n",
    "Formally, we are given a dataset of pairs\n",
    "$$\n",
    "\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(N)}, y^{(N)})\\},\n",
    "$$\n",
    "where each input $ x^{(n)} $ (e.g. an image) has an associated label $ y^{(n)} $ (e.g. the digit it represents).\n",
    "The goal is to **learn a function** $ f_\\theta(x) $ parameterized by weights $ \\theta $ that predicts the correct label $ y $ for new, unseen inputs.\n",
    "\n",
    "Within supervised learning, two main task types are typically distinguished:\n",
    "\n",
    "- **Classification:** the output variable $ y $ is **discrete** (e.g. categories such as digit 0‚Äì9, or ‚Äúawake‚Äù vs ‚Äúasleep‚Äù).\n",
    "- **Regression:** the output variable $ y $ is **continuous** (e.g. predicting a firing rate, temperature, or probability).\n",
    "\n",
    "In this exercise, we focus on a **classification** task: learning to assign each image to one category (digits 0/1).\n",
    "\n",
    "#### **The MNIST Dataset**\n",
    "\n",
    "The **MNIST** (Modified National Institute of Standards and Technology) dataset [2] is one of the most iconic benchmarks in machine learning.\n",
    "It consists of:\n",
    "- **60,000 training images** and **10,000 test images** of handwritten digits from **0 to 9**.\n",
    "- Each image is **28√ó28 pixels**, grayscale, and labeled with the correct digit.\n",
    "\n",
    "MNIST is often considered the ‚ÄúHello World‚Äù of image classification.\n",
    "Its simplicity and balanced structure make it ideal for testing new algorithms -- from classical models like the Perceptron and logistic regression to deep convolutional networks [3].\n",
    "\n",
    "Here, we will start from the simplest possible supervised task: classifying between **two digits only** (0 vs 1) using a simple **distance-based classifier** and the **Perceptron algorithm** [2].\n",
    "\n",
    "The next cell will download and prepare the MNIST dataset for binary classification between digits 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed5f9a55a656c14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:52:49.520928Z",
     "start_time": "2025-11-12T16:52:49.363964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded dataset from `mnist_data`\n",
      " X (images) shape : (14780, 28, 28)\n",
      "Ô∏è y (labels) shape: (14780,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.isdir(\"mnist_data\"):\n",
    "    raise FileNotFoundError(\n",
    "        \"‚ùå The 'data' folder was not found. Please make sure you are in the correct working directory.\"\n",
    "    )\n",
    "\n",
    "def load_mnist_data(data_path, digits=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Load MNIST arrays from .npy files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to directory containing `mnist_x.npy` and `mnist_y.npy`.\n",
    "    digits : None, int, tuple or list-like of ints, optional\n",
    "        If None, return the full dataset.\n",
    "        If an int, tuple, or list-like, return only the samples whose labels are in `digits`.\n",
    "        Preferred form is a tuple, e.g. (0, 1).\n",
    "    verbose : bool, optional\n",
    "        If True, print shapes of the returned arrays.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (images, labels)\n",
    "        - images : ndarray, shape (N, 28, 28) for full dataset or (N_sub, 28, 28) for a subset\n",
    "        - labels : ndarray, shape (N,) values 0-9 or (N_sub,) restricted to `digits`\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If `mnist_x.npy` or `mnist_y.npy` are not found in `data_path`.\n",
    "    TypeError\n",
    "        If `digits` is not None and cannot be converted to a tuple of ints.\n",
    "    \"\"\"\n",
    "    # Build file paths\n",
    "    mnist_x_path = os.path.join(data_path, 'mnist_x.npy')\n",
    "    mnist_y_path = os.path.join(data_path, 'mnist_y.npy')\n",
    "\n",
    "    # Validate files exist\n",
    "    if not os.path.exists(mnist_x_path) or not os.path.exists(mnist_y_path):\n",
    "        raise FileNotFoundError(f\"MNIST .npy files not found in {data_path}\")\n",
    "\n",
    "    # Load full MNIST arrays\n",
    "    mnist_x = np.load(mnist_x_path)  # expected shape (70k, 28, 28)\n",
    "    mnist_y = np.load(mnist_y_path)  # expected shape (70k,) values 0-9\n",
    "\n",
    "    # If no digits specified, return full dataset\n",
    "    if digits is None:\n",
    "        ret_x, ret_y = mnist_x, mnist_y\n",
    "    else:\n",
    "        # Normalize digits argument to a tuple of ints (preferred form)\n",
    "        if np.isscalar(digits):\n",
    "            digits = (int(digits),)\n",
    "        elif isinstance(digits, (list, tuple, np.ndarray)):\n",
    "            digits = tuple(int(d) for d in digits)\n",
    "        else:\n",
    "            raise TypeError(\"`digits` must be None, an int, or an iterable of ints (e.g. (0, 1))\")\n",
    "\n",
    "        # Build mask for requested digits and return subset\n",
    "        mask = np.isin(mnist_y, digits)\n",
    "        ret_x = mnist_x[mask]\n",
    "        ret_y = mnist_y[mask]\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\n‚úÖ Loaded dataset from `{data_path}`\"\n",
    "            f\"\\n X (images) shape : {ret_x.shape}\"\n",
    "            f\"\\nÔ∏è y (labels) shape: {ret_y.shape}\"\n",
    "        )\n",
    "\n",
    "    return ret_x, ret_y\n",
    "\n",
    "x_01, y_01 = load_mnist_data(\"mnist_data\", digits=(0,1), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa9a3e6aa5b45d",
   "metadata": {},
   "source": [
    "The following cell visualizes some examples of the digits 0 and 1 from the loaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "831cc0acfefd17e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T16:52:49.885463Z",
     "start_time": "2025-11-12T16:52:49.525756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 787.402x787.402 with 25 Axes>"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = 1/2.54  # centimeters in inches\n",
    "\n",
    "def plot_mnist_digits(x, y=None, start=0, randomise=False, seed=None, n=25):\n",
    "    \"\"\"\n",
    "    Plot `n` images from `x` in a grid. By default plots 25 images in a 5x5 panel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like, shape (N, 28, 28) or (N, n_features)\n",
    "        Image array.\n",
    "    y : array-like, optional\n",
    "        Labels corresponding to images.\n",
    "    start : int\n",
    "        Index of first image to plot (ignored when randomize=True).\n",
    "    randomise : bool\n",
    "        If True select `n` random images.\n",
    "    seed : int or None\n",
    "        Random seed for reproducible selection.\n",
    "    n : int\n",
    "        Number of images to plot (default 16).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    N = x.shape[0]\n",
    "\n",
    "    # determine grid shape (try square-ish)\n",
    "    nrows = int(np.floor(np.sqrt(n)))\n",
    "    ncols = int(np.ceil(n / nrows))\n",
    "\n",
    "    # select indices\n",
    "    if randomise:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        indices = rng.choice(N, size=n, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(start, start + n) % N\n",
    "\n",
    "    cm = 1/2.54  # centimeters in inches\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(20*cm, 20*cm))\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        ax = axes[i]\n",
    "        if x.ndim == 3:\n",
    "            img = x[idx]\n",
    "        else:\n",
    "            # assume flattened; try to reshape to 28x28 if possible\n",
    "            try:\n",
    "                img = x[idx].reshape(28, 28)\n",
    "            except Exception:\n",
    "                img = x[idx]\n",
    "        ax.imshow(img, cmap=sns.color_palette(\"Grays\", as_cmap=True))\n",
    "        if y is not None:\n",
    "            ax.set_title(f'{y[idx]}', fontsize=12)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # turn off any unused subplots\n",
    "    for j in range(len(indices), len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_mnist_digits(x_01, y_01, randomise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a205e3a152497d",
   "metadata": {},
   "source": [
    "## 1 - Data Exploration and Visualization\n",
    "\n",
    "Before building classifiers, we first need to **understand the data**. In this section, we will:\n",
    "- Split the dataset into **training** and **test** sets\n",
    "- Compute and interpret the **average image** for each class\n",
    "\n",
    "The MNIST subset we will use contains **14,780 images** of handwritten digits *0* and *1*.\n",
    "We will use the first **6/7** of the images for training and the remaining **1/7** for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191271a0e3e834b9",
   "metadata": {},
   "source": [
    "\n",
    "#### üéØ Question 1a. Train/Test Split\n",
    "\n",
    "Write a function that takes as input $f\\in(0,1)$ and splits the data the arrays `x_01` and `y_01` into a **training set** (first fraction $ f $ of the samples and labels) and a **test set** (remaining fraction $ 1 - f$).\n",
    "\n",
    "Store the results as:\n",
    "- `x_01_train`, `y_01_train` -- training data and labels\n",
    "- `x_01_test`, `y_01_test` -- test data and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adcfb8b6520f70f",
   "metadata": {},
   "source": [
    "#### üéØ Question 1b. Computing Templates and distance\n",
    "\n",
    "A simple way to understand what distinguishes the two classes is to compute their **average images** (also called *templates*).\n",
    "\n",
    "Compute the average image for each digit:\n",
    "$$\n",
    "\\boldsymbol{m}_0 = \\frac{1}{N_0} \\sum_{n | y^{(n)} = 0} \\boldsymbol{x}^{(n)}, \\quad\n",
    "\\boldsymbol{m}_1 = \\frac{1}{N_1} \\sum_{n | y^{(n)} = 1} \\boldsymbol{x}^{(n)}\n",
    "$$\n",
    "\n",
    "where $N_0$ and $N_1$ are the number of training examples for each class. Store these as `m0` and `m1` -- both should have shape (28, 28). Then compute their **difference** -- which is also a matrix (28, 28) -- $$\\boldsymbol{d} = \\boldsymbol{m}_1 - \\boldsymbol{m}_0\\ .$$ Plot `m0`, `m1`, `d` side by side using `plt.imshow()` -- use the palette `RdBu` or `bwr` for `d`.\n",
    "\n",
    "Interpretation:\n",
    "- Positive values indicate pixels where digit 1 tends to be brighter than digit 0\n",
    "- Negative values indicate pixels where digit 0 tends to be brighter than digit 1\n",
    "- This difference vector $\\boldsymbol{d}$ captures the **direction** in pixel space that best separates the two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8421dc0af6c0d1",
   "metadata": {},
   "source": [
    "#### üéØ Question 1c. Flatten Images\n",
    "\n",
    "Then, create **flattened versions** of the image arrays, reshaping each 28√ó28 image into a 784-dimensional vector:\n",
    "- `x_train_flat` -- shape `(n_train, 784)`\n",
    "- `x_test_flat` -- shape `(n_test, 784)`\n",
    "- `d` -- shape `(784,)`\n",
    "\n",
    "üí° *Hint:* Use the`.reshape()` method in NumPy. Each 28√ó28 image becomes a 1D vector of length 784.\n",
    "\n",
    "Finally, print the shapes of all arrays to verify that your split is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a85b153be6cb",
   "metadata": {},
   "source": [
    "## 2 - Linear Classifier with Threshold Optimization\n",
    "\n",
    "We have seen that the difference vector $\\boldsymbol{d} = \\boldsymbol{m}_1 - \\boldsymbol{m}_0$ provides a natural direction for separating the two classes. In this section, we formalize this into a **linear classifier** and optimize its decision threshold.\n",
    "\n",
    "#### **The Linear Classifier**\n",
    "\n",
    "We have seen that the difference vector $\\boldsymbol{d} = \\boldsymbol{m}_1 - \\boldsymbol{m}_0$ provides a natural direction for separating the two classes. In this section, we formalize this into a **linear classifier** and optimize its decision threshold.\n",
    "\n",
    "This linear classifier will make predictions based on a simple rule:\n",
    "$$\n",
    "\\hat{y}^{(n)} = \\begin{cases}\n",
    "1 & \\text{if } \\boldsymbol{d} \\cdot \\boldsymbol{x}^{(n)} > \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $\\theta$ is a **threshold** (also called **bias**), and $\\boldsymbol{x}^{(n)}$ is the $n$-th training example. The classifier separates the input space into two regions using a **decision boundary**: the hyperplane where $\\boldsymbol{d} \\cdot \\boldsymbol{x} = \\theta$.\n",
    "\n",
    "Note that $\\boldsymbol{d} \\cdot \\boldsymbol{x}^{(n)} = \\boldsymbol{m}_1 \\cdot \\boldsymbol{x}^{(n)} - \\boldsymbol{m}_0 \\cdot \\boldsymbol{x}^{(n)}$, therefore the classifier effectively measures how much closer an input $\\boldsymbol{x}^{(n)}$ is to the average image of class 1 versus class 0. The threshold $\\theta$ determines where we draw the line between the two classes.\n",
    "\n",
    "The key question is: **what is the optimal threshold $\\theta^*$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7715954dd57ed5b",
   "metadata": {},
   "source": [
    "#### üéØ Question 2a. Computing the scores\n",
    "Compute the **scores** for all training examples:\n",
    "$$\n",
    "s^{(n)} = \\boldsymbol{x}^{(n)} \\cdot \\boldsymbol{d}\n",
    "$$\n",
    "Then, use the known train labels to plot a histogram of the scores for the two classes (0 and 1) using different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312580b8847a1706",
   "metadata": {},
   "source": [
    "#### üéØ Question 2b. Linear Classifier and Classification Error\n",
    "\n",
    "We now turn the score $ s = \\boldsymbol{d} \\cdot \\boldsymbol{x} $ into an actual **decision rule** for classifying digits.\n",
    "\n",
    "**2.b.i** / Write a function `linear_classifier_predict(x, d, theta)` that:\n",
    "- Takes as input:\n",
    "  - `x`: array of flattened images, shape (n_samples, 784)\n",
    "  - `d`: direction vector, shape (784,)\n",
    "  - `theta`: scalar threshold\n",
    "- Returns:\n",
    "  - `y_pred`: predicted labels (0 or 1), shape (n_samples,)\n",
    "\n",
    "The prediction rule is: predict 1 if $\\boldsymbol{d} \\cdot \\boldsymbol{x} > \\theta$, else predict 0.\n",
    "\n",
    "**2.b.ii** / To evaluate classifier performance, we need to measure how often it makes mistakes. Write a function `compute_error_rate(y_true, y_pred)` that takes true labels `y_true` and predicted labels `y_pred` and returns the **error rate** (fraction of mistakes):\n",
    "$$\n",
    "\\text{Error} = \\frac{1}{N} \\sum_{n=1}^{N} 1 - \\delta_{y_{\\text{pred}}^{(n)}\\ ,\\ y_{\\text{true}}^{(n)}}\n",
    "$$\n",
    "where $\\delta_{a,b}$ is the Kroenecker function (1 if $a=b$, 0 otherwise).\n",
    "\n",
    "\n",
    "**2.b.iii** / Using $\\theta = 0$, compute and print the training error and test error.\n",
    "Do these numbers tell you the classifier is working well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec1deb6ed42558",
   "metadata": {},
   "source": [
    "#### üéØ Question 2c. Finding the Optimal Threshold\n",
    "\n",
    "The threshold $\\theta$ is a hyperparameter we can optimize. We will search for the threshold that minimizes the training error.\n",
    "\n",
    "**Algorithm**: Grid search for optimal threshold\n",
    "\n",
    "1. Create an array `theta_values` of 200 evenly-spaced thresholds between the minimum and maximum projection scores on the training set\n",
    "2. For each threshold in `theta_values`, compute the training error and store in an array `errors`\n",
    "3. Find the optimal threshold: `theta_star = theta_values[np.argmin(errors)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc168017edd555",
   "metadata": {},
   "source": [
    "#### üéØ Question 2d. **Visualization and Reporting**.\n",
    "\n",
    "Plot the training error as a function of $\\theta$ with clear axis labels. Mark $\\theta^*$ on the plot with a vertical dashed line.\n",
    "\n",
    "Plot the histograms in (2.a) again, this time adding a vertical line at $\\theta^*$ to visualize the decision boundary.\n",
    "\n",
    "Compute and report the training and test error at $\\theta^*$.\n",
    "\n",
    "**Interpretation**: What shape does the error curve have? Why? How do the training and test errors at $\\theta^*$ compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a189d3cabe203ce9",
   "metadata": {},
   "source": [
    "## 3 - The Perceptron Algorithm\n",
    "\n",
    "So far, we have used a **fixed direction** $\\boldsymbol{d} = \\boldsymbol{m}_1 - \\boldsymbol{m}_0$ derived from the class averages. While effective, this approach has limitations: it doesn't adapt to misclassified examples and it treats all training samples equally (no learning from mistakes).\n",
    "\n",
    "The **Perceptron algorithm** [1], introduced by Frank Rosenblatt in 1958, addresses these issues through **iterative learning**. Instead of computing a fixed direction, the perceptron **learns** a weight vector $\\boldsymbol{J}$ by repeatedly correcting its mistakes on the training data. See sec. 7.1 in [4].\n",
    "\n",
    "#### **The Perceptron Learning Rule**\n",
    "\n",
    "The core idea is simple: **whenever the classifier makes a mistake, adjust the weights to correct it**.\n",
    "\n",
    "Let us assume that the labels are encoded as $y^{(m)} \\in \\{+1, -1\\}$ for the two classes (e.g., +1 for digit 1 and -1 for digit 0). In the standard formulation, the perceptron must handle two cases separately:\n",
    "- If $y^{(m)} = +1$ but $\\boldsymbol{J} \\cdot \\boldsymbol{x}^{(m)} \\leq 0$: add $\\boldsymbol{x}^{(m)}$ to $\\boldsymbol{J}$\n",
    "- If $y^{(m)} = -1$ but $\\boldsymbol{J} \\cdot \\boldsymbol{x}^{(m)} \\geq 0$: subtract $\\boldsymbol{x}^{(m)}$ from $\\boldsymbol{J}$\n",
    "\n",
    "Both cases can be unified by the update rule: $\\boldsymbol{J} \\leftarrow \\boldsymbol{J} + y^{(m)} \\boldsymbol{x}^{(m)}$.\n",
    "\n",
    "#### **The Signed Pattern Formulation**\n",
    "\n",
    "In this tutorial, we use an equivalent but more elegant formulation. We **pre-multiply each input by its label** to create **signed patterns**:\n",
    "$$\n",
    "\\boldsymbol{\\eta}^{(m)} = y^{(m)} \\cdot \\boldsymbol{x}^{(m)}\n",
    "$$\n",
    "\n",
    "**The key insight**: With this transformation, correct classification has a **single unified condition** for all patterns:\n",
    "$$\n",
    "\\boldsymbol{J} \\cdot \\boldsymbol{\\eta}^{(m)} > 0 \\quad \\text{(correct classification, regardless of class)}\n",
    "$$\n",
    "\n",
    "**Why?** The label sign automatically flips the direction:\n",
    "- For $y^{(m)} = +1$: we need $\\boldsymbol{J} \\cdot \\boldsymbol{x}^{(m)} > 0$, which is $\\boldsymbol{J} \\cdot \\boldsymbol{\\eta}^{(m)} > 0$ ‚úì\n",
    "- For $y^{(m)} = -1$: we need $\\boldsymbol{J} \\cdot \\boldsymbol{x}^{(m)} < 0$, which becomes $\\boldsymbol{J} \\cdot (-\\boldsymbol{x}^{(m)}) > 0$, i.e., $\\boldsymbol{J} \\cdot \\boldsymbol{\\eta}^{(m)} > 0$ ‚úì\n",
    "\n",
    "The perceptron's goal simplifies to: **find $\\boldsymbol{J}$ with positive dot product with all signed patterns**.\n",
    "\n",
    "#### **The Stability (Margin) Concept**\n",
    "\n",
    "For each pattern $m$, we define its **stability** as:\n",
    "$$\n",
    "c^{(m)} = \\boldsymbol{J} \\cdot \\boldsymbol{\\eta}^{(m)}\n",
    "$$\n",
    "\n",
    "The stability measures how confidently pattern $m$ is classified:\n",
    "- If $c^{(m)} > 0$: pattern is correctly classified\n",
    "- If $c^{(m)} \\leq 0$: pattern is **misclassified**\n",
    "\n",
    "The **minimum stability** across all patterns is:\n",
    "$$\n",
    "c_{\\min} = \\min_{m=1,\\dots,M} \\boldsymbol{J} \\cdot \\boldsymbol{\\eta}^{(m)}\n",
    "$$\n",
    "\n",
    "This represents the **worst-case pattern** -- the one that is misclassified or closest to being misclassified.\n",
    "\n",
    "#### **The Perceptron Update Rule**\n",
    "\n",
    "At each iteration, the algorithm:\n",
    "1. Computes stabilities $c^{(m)} = \\boldsymbol{J} \\cdot \\boldsymbol{\\eta}^{(m)}$ for all patterns\n",
    "2. Identifies the pattern $m_t$ with **minimum stability** (most violated)\n",
    "3. Updates the weight vector:\n",
    "$$\n",
    "\\boldsymbol{J}^{(t+1)} = \\boldsymbol{J}^{(t)} + \\boldsymbol{\\eta}^{(m_t)}\n",
    "$$\n",
    "\n",
    "**Why does this work?** After the update, the stability of the selected pattern increases:\n",
    "$$\n",
    "\\boldsymbol{J}^{(\\text{new})} \\cdot \\boldsymbol{\\eta}^{(m_t)} = \\underbrace{(\\boldsymbol{J}^{(\\text{old})} + \\boldsymbol{\\eta}^{(m_t)})}_{\\text{updated } \\boldsymbol{J}} \\cdot \\boldsymbol{\\eta}^{(m_t)} = \\underbrace{\\boldsymbol{J}^{(\\text{old})} \\cdot \\boldsymbol{\\eta}^{(m_t)}}_{\\text{old stability}} + \\underbrace{\\|\\boldsymbol{\\eta}^{(m_t)}\\|^2}_{> 0}\n",
    "$$\n",
    "\n",
    "The update **always improves** the classification of the most problematic pattern.\n",
    "\n",
    "**Perceptron Convergence Theorem**: If the training data is **linearly separable**, the perceptron algorithm is guaranteed to converge to a solution (perfect classification) in a **finite number of steps**. This is remarkable: even though we're only fixing one pattern at a time, the algorithm eventually finds weights that classify **all** patterns correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7b57a06a766dc",
   "metadata": {},
   "source": [
    "#### üéØ Question 3a. Implementing the Basic Perceptron Algorithm\n",
    "\n",
    "We will implement the perceptron in two stages: first a basic version, then add history tracking.\n",
    "\n",
    "**Stage 1: Core Algorithm (without history tracking)**\n",
    "\n",
    "Implement a function `perceptron_signed(X_train, y_train, max_updates=2000)` following this pseudocode:\n",
    "```\n",
    "Algorithm: Perceptron with Signed Patterns\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Input: X_train (n_samples, n_features), y_train (n_samples) in {-1, +1}\n",
    "Output: J (weight vector, normalized)\n",
    "\n",
    "1. Re-label patterns y_train to {-1, +1} .\n",
    "\n",
    "2. Create signed patterns:\n",
    "   eta = y_train[:, None] * X_train    # shape (n_samples, n_features)\n",
    "\n",
    "3. Initialize:\n",
    "   J = zeros(n_features)\n",
    "   n_updates = 0\n",
    "\n",
    "4. Main loop (while n_updates < max_updates):\n",
    "   a) Compute stabilities for all patterns:\n",
    "      stabilities = eta @ J\n",
    "\n",
    "   b) Find worst pattern (minimum stability):\n",
    "      m_worst = argmin(stabilities)\n",
    "      c_min = stabilities[m_worst]\n",
    "\n",
    "   c) Perceptron update:\n",
    "      J = J + eta[m_worst, :]\n",
    "      n_updates += 1\n",
    "\n",
    "   d) Optional: check convergence\n",
    "      if c_min > 0 and all patterns classified correctly:\n",
    "          break\n",
    "\n",
    "5. Normalize final weights:\n",
    "   J = J / ||J||\n",
    "\n",
    "6. Return J, n_updates\n",
    "```\n",
    "Print the number of updates performed and verify that `J` has unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc13b847ddbe90",
   "metadata": {},
   "source": [
    "#### üéØ Question 3b. Testing the Basic Perceptron\n",
    "\n",
    "Run the perceptron algorithm on your training data with your **original labels** (0 and 1).\n",
    "\n",
    "Using the weight vector `J` and your `linear_classifier_predict()` function from Section 2 (which predicts 0 or 1), compute and print the training and test error rates.\n",
    "\n",
    "‚ÑπÔ∏è **Important Note**: The $\\{-1, +1\\}$ encoding is **only used internally during learning** to construct the signed patterns $\\boldsymbol{\\eta}^{(m)}$ and update the weights. Once training is complete, the learned weight vector $\\boldsymbol{J}$ is simply a **direction in the 784-dimensional pixel space**, just like $\\boldsymbol{d}$ was in Sections 1-2.\n",
    "\n",
    "To make predictions, we use the same rule as before:\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}\n",
    "1 & \\text{if } \\boldsymbol{J} \\cdot \\boldsymbol{x} > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The signed pattern formulation is a mathematical tool for **learning** the direction $\\boldsymbol{J}$, but the final classifier is identical in form to the linear classifiers from Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b22058790f7b8",
   "metadata": {},
   "source": [
    "#### üéØ Question 3c. Visualizing Class Separation\n",
    "\n",
    "Visualize how well the learned direction $\\boldsymbol{J}$ separates the two classes.\n",
    "\n",
    "Compute projection scores $\\boldsymbol{J} \\cdot \\boldsymbol{x}$ for training and test data.\n",
    "\n",
    "Plot overlapping histograms (normalized densities) for each class, marking the decision boundary at score = 0.\n",
    "\n",
    "**Interpretation**:\n",
    "- Are the classes perfectly separated in training data? In test data?\n",
    "- Compare with Section 1d histograms (using $\\boldsymbol{d} = \\boldsymbol{m}_1 - \\boldsymbol{m}_0$). Which direction achieves better separation?\n",
    "- Does the test overlap match your test error rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765fa37aacebeef",
   "metadata": {},
   "source": [
    "#### üéØ Question 3d. Adding History Tracking\n",
    "\n",
    "To understand how the perceptron learns, we need to track its progress during training.\n",
    "\n",
    "**Extend your `perceptron_signed()` function** to add history tracking. When `track_history=True`, record at each iteration:\n",
    "\n",
    "1. Normalized minimum stability $c_{\\min} / \\|\\boldsymbol{J}\\|$ (track BEFORE the update):\n",
    "\n",
    "2. **Training error** (track AFTER the update):\n",
    "   - Use your `linear_classifier_predict()` and `compute_error_rate()` functions\n",
    "\n",
    "Run the Perceptron learning with tracking enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4fc5e8677e11e1",
   "metadata": {},
   "source": [
    "#### üéØ Question 3e. Visualizing Convergence\n",
    "\n",
    "Create two plots to understand how the perceptron learns over iterations: 1. Training error vs. iteration 2. Normalized minimum stability vs. iteration.\n",
    "\n",
    "üí° *Hint*: Use `plt.xscale('log')` for logarithmic x-axis to see both early and late training dynamics.\n",
    "\n",
    "**Interpretation questions**:\n",
    "1. Does training error reach 0%? At approximately which iteration?\n",
    "2. Does minimum stability become positive? What does this mean for classification?\n",
    "3. Where does most learning happen: early iterations (< 100) or later (> 1000)?\n",
    "4. What is the relationship between stability becoming positive and error reaching zero?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
